{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW/wqBXevgg1Gvedc8ndLJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudharshanreddy4567/Sentemintal_analysis/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUakumf_WLo6"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load the Sentiment140 dataset\n",
        "# Dataset can be downloaded from Kaggle: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
        "# The dataset has the following columns:\n",
        "# 0 - target (0 = negative, 2 = neutral, 4 = positive)\n",
        "# 1 - ids\n",
        "# 2 - date\n",
        "# 3 - flag\n",
        "# 4 - user\n",
        "# 5 - text\n",
        "\n",
        "# Define column names\n",
        "columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
        "\n",
        "# Load the dataset (adjust the path as needed)\n",
        "try:\n",
        "    df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', names=columns)\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Please download the dataset from Kaggle and update the file path.\")\n",
        "    # Sample fallback with small data (in case dataset not found)\n",
        "    data = {\n",
        "        'text': ['I love this product!', 'This is terrible.', 'It is okay, not great.'],\n",
        "        'target': [4, 0, 2]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "# Map target values to sentiment categories\n",
        "# In Sentiment140, 0=negative, 2=neutral, 4=positive\n",
        "df['sentiment'] = df['target'].map({0: 'negative', 2: 'neutral', 4: 'positive'})\n",
        "\n",
        "# For this example, let's work with a smaller sample for faster processing\n",
        "df = df.sample(frac=0.1, random_state=42)  # Using 10% of the data for demonstration\n",
        "\n",
        "# Text preprocessing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    # Join tokens back to string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to text data\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X = df['cleaned_text']\n",
        "y = df['target'].replace({4: 1, 2: 1, 0: 0})  # Converting to binary: positive/neutral=1, negative=0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000)  # Using top 5000 features\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Convert sparse matrices to dense arrays\n",
        "X_train_tfidf = X_train_tfidf.toarray()\n",
        "X_test_tfidf = X_test_tfidf.toarray()\n",
        "\n",
        "# Build Artificial Neural Network (ANN) model\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_tfidf, y_train,\n",
        "    validation_data=(X_test_tfidf, y_test),\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_test_tfidf)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nModel Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['negative', 'positive/neutral']))\n",
        "\n",
        "# Example prediction\n",
        "sample_texts = [\n",
        "    \"I love this product! It's amazing!\",\n",
        "    \"This is the worst experience ever.\",\n",
        "    \"The service was okay, nothing special.\"\n",
        "]\n",
        "\n",
        "# Preprocess sample texts\n",
        "cleaned_samples = [preprocess_text(text) for text in sample_texts]\n",
        "sample_tfidf = tfidf.transform(cleaned_samples).toarray()\n",
        "\n",
        "# Predict sentiment\n",
        "sample_preds = (model.predict(sample_tfidf) > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for text, pred in zip(sample_texts, sample_preds):\n",
        "    sentiment = 'positive/neutral' if pred == 1 else 'negative'\n",
        "    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")"
      ]
    }
  ]
}